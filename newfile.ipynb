{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Google Colab installs (if running in Google Colab)\n",
    "import os\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
    "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
    "    !pip install PyMuPDF # for reading PDFs with Python\n",
    "    !pip install tqdm # for progress bars\n",
    "    !pip install sentence-transformers # for embedding models\n",
    "    !pip install accelerate # for quantization model loading\n",
    "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
    "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "pdf_path = \"human-nutrition-text.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File dose not exist. Downloading...\")\n",
    "\n",
    "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "  filename = pdf_path\n",
    "\n",
    "  response = requests.get(url)  \n",
    "\n",
    "  if response.status_code == 200:\n",
    "\n",
    "    with open(filename,\"wb\") as file:\n",
    "      file.write(response.content)\n",
    "    print(f\"The file has been downloaded and saved as {pdf_path}\")\n",
    "  else:\n",
    "    print(f\"Failed to download the file. Status code is {response.statys_code}\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} Already Exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text : str) -> str:\n",
    "  cleaned_text = text.replace(\"\\n\",\" \").strip()\n",
    "  return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path : str) -> list[dict]:\n",
    "  doc = fitz.open(pdf_path)\n",
    "  pages_and_texts = []\n",
    "\n",
    "  for page_number,page in tqdm(enumerate(doc)):\n",
    "    text = page.get_text()\n",
    "    text = text_formatter(text)\n",
    "    pages_and_texts.append(\n",
    "        {\"page_number\":page_number - 41,\n",
    "         \"page_char_count\":len(text),\n",
    "         \"page_word_count\":len(text.split(\" \")),\n",
    "         \"page_sentence_raw_count\":len(text.split(\". \")),\n",
    "         \"page_token_count\": len(text)/4,\n",
    "         \"text\": text})\n",
    "  return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path)\n",
    "pages_and_texts[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "  item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "  item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "  item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence_chunk_size = 10\n",
    "\n",
    "def split_test(input_list : list, slice_size:int) -> list[list[str]]:\n",
    "\n",
    "  return [input_list[ i : i + slice_size] for i in range( 0 , len(input_list),slice_size)]\n",
    "\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "  item[\"sentence_chunks\"] = split_test(item[\"sentences\"],num_sentence_chunk_size)\n",
    "\n",
    "  item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pages_and_chunks = []\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "  for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "    chunk_dict = {}\n",
    "    chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "    joined_sentence_chunk = \" \".join(sentence_chunk).replace(\"  \",\" \").strip()\n",
    "    joined_sentence_chunk = re.sub(r'\\.([A-Z])',r'. \\1',joined_sentence_chunk)\n",
    "    chunk_dict['sentence_chunk'] = joined_sentence_chunk\n",
    "\n",
    "    chunk_dict['chunk_character_count'] = len(joined_sentence_chunk)\n",
    "    chunk_dict['chunk_word_count'] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "    chunk_dict['chunk_token_count'] = len(joined_sentence_chunk)/4\n",
    "\n",
    "    pages_and_chunks.append(chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
